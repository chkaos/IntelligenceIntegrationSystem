# CrawlerGenerated.py - This code is generated by CrawlerPlayground
from IntelligenceCrawler.CrawlPipeline import *

# === Fetcher init parameters ===
d_fetcher_init_param = {'log_callback': log_cb, 'proxy': 'http://127.0.0.1:10809', 'timeout_s': 10}
e_fetcher_init_param = {'log_callback': log_cb, 'proxy': 'http://127.0.0.1:10809', 'timeout_s': 20, 'stealth': True, 'pause_browser': False, 'render_page': True}

# === Crawl parameters ===
entry_point = ['https://www.nhk.or.jp/rss/news/cat1.xml', 'https://www.nhk.or.jp/rss/news/cat4.xml', 'https://www.nhk.or.jp/rss/news/cat5.xml', 'https://www.nhk.or.jp/rss/news/cat6.xml']
start_date = None
end_date = None
d_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0}
e_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 40, 'scroll_pages': 0}
extractor_kwargs = {}
channel_filter_list = []

def run_pipeline(
        article_filter = lambda url: True,
        content_handler = save_article_to_disk,
        exception_handler = lambda url, exception: None
):
    # === 1. Initialize Components ===
    d_fetcher = RequestsFetcher(**d_fetcher_init_param)
    e_fetcher = PlaywrightFetcher(**e_fetcher_init_param)
    discoverer = RSSDiscoverer(fetcher=d_fetcher, verbose=True)
    extractor = TrafilaturaExtractor(verbose=True)

    # === 2. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date,
                               fetcher_kwargs=d_fetcher_kwargs)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list),
        fetcher_kwargs=d_fetcher_kwargs)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=article_filter,
        content_handler=content_handler,
        exception_handler=exception_handler,
        fetcher_kwargs=e_fetcher_kwargs,
        extractor_kwargs=extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())


# -------------------------------------------- Manual Code Start --------------------------------------------

import random
from playwright.sync_api import Page
from ServiceEngine import ServiceContext
from MyPythonUtility.easy_config import EasyConfig
from Workflow.CommonFlowUtility import CrawlContext
from Workflow.CommonFeedsCrawFlow import build_crawl_ctx_by_config
from Workflow.IntelligenceCrawlFlow import (
    intelligence_crawler_result_handler,
    intelligence_crawler_fileter, \
    intelligence_crawler_exception_handler)

NAME = 'nhk'
config: EasyConfig | None = None
crawl_context: CrawlContext | None = None


def conditional_click_nhk(page: Page):
        # 整个弹窗的ID
        modal_selector = "#erpc-half-modal"
        # 按钮的文本
        button_text = "確認しました / I understand"

        # 使用 page.locator() 结合 get_by_text，定位器更加稳定
        modal_locator = page.locator(modal_selector)
        button_locator = modal_locator.get_by_text(button_text, exact=True)
        # --------------------

        # 1. 检查按钮是否可见（设置超时为 5 秒）
        is_present = button_locator.is_visible(timeout=5000)

        if is_present:
            print(f"发现弹窗（{modal_selector}），正在点击按钮: {button_text}")
            # 2. 点击按钮
            button_locator.click()

            # 3. 等待整个弹窗消失 (state="hidden")，确保页面可操作
            try:
                page.wait_for_selector(modal_selector, state="hidden", timeout=10000)
                print("弹窗已成功关闭。")
            except Exception:
                print("警告：弹窗未在指定时间内消失。")
        else:
            print("未发现弹窗，直接继续抓取。")


def module_init(service_context: ServiceContext):
    global config
    global crawl_context
    config = service_context.config
    crawl_context = build_crawl_ctx_by_config(NAME, config)

    e_fetcher_kwargs['post_extra_action'] = conditional_click_nhk


def start_task(stop_event):
    # Crawl main process
    run_pipeline(
        article_filter=partial(intelligence_crawler_fileter, context=crawl_context, levels=[NAME]),
        content_handler=partial(intelligence_crawler_result_handler, context=crawl_context, levels=[NAME]),
        exception_handler=partial(intelligence_crawler_exception_handler, context=crawl_context, levels=[NAME])
    )
    # Check and submit cached data.
    crawl_context.submit_cached_data(10)
    # Randomly delay for next crawl.
    CrawlContext.wait_interruptibly(random.randint(10, 15) * 60, stop_event)

# --------------------------------------------- Manual Code End ---------------------------------------------
