# CrawlerGenerated.py - This code is generated by CrawlerPlayground

from IntelligenceCrawler.CrawlPipeline import *

# === Fetcher init parameters ===
d_fetcher_init_param = {'log_callback': log_cb, 'proxy': None, 'timeout_s': 10, 'stealth': True, 'pause_browser': False, 'render_page': False}
e_fetcher_init_param = {'log_callback': log_cb, 'proxy': None, 'timeout_s': 20, 'stealth': True, 'pause_browser': False, 'render_page': True}

# === Crawl parameters ===
entry_point = ['https://tass.com/world', 'https://tass.com/emergencies', 'https://tass.com/politics', 'https://tass.com/economy', 'https://tass.com/defense', 'https://tass.com/society']
start_date = None
end_date = None
d_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 10, 'scroll_pages': 5}
e_fetcher_kwargs = {'wait_until': 'networkidle', 'wait_for_selector': None, 'wait_for_timeout_s': 20, 'scroll_pages': 0}
extractor_kwargs = {}
channel_filter_list = []

def run_pipeline(
        article_filter = lambda url: True,
        content_handler = save_article_to_disk,
        exception_handler = lambda url, exception: None
):
    # === 1. Initialize Components ===
    d_fetcher = PlaywrightFetcher(**d_fetcher_init_param)
    e_fetcher = PlaywrightFetcher(**e_fetcher_init_param)
    discoverer = ListPageDiscoverer(fetcher=d_fetcher, verbose=True, manual_specified_signature=None)
    extractor = TrafilaturaExtractor(verbose=True)

    # === 2. Build pipeline ===
    pipeline = CrawlPipeline(
        d_fetcher=d_fetcher,
        discoverer=discoverer,
        e_fetcher=e_fetcher,
        extractor=extractor,
        log_callback=log_cb
    )

    # Step 1: Discover all channels
    pipeline.discover_channels(entry_point, start_date, end_date,
                               fetcher_kwargs=d_fetcher_kwargs)

    # Step 2: Discover and fetch articles (populates pipeline.contents)
    pipeline.discover_articles(channel_filter=partial(
        common_channel_filter, channel_filter_list=channel_filter_list),
        fetcher_kwargs=d_fetcher_kwargs)

    # Step 3: Extract content and run handlers
    pipeline.extract_articles(
        article_filter=article_filter,
        content_handler=content_handler,
        exception_handler=exception_handler,
        fetcher_kwargs=e_fetcher_kwargs,
        extractor_kwargs=extractor_kwargs
    )


if __name__ == "__main__":
    try:
        run_pipeline()
    except Exception as e:
        print(str(e))
        print(traceback.format_exc())


# -------------------------------------------- Manual Code Start --------------------------------------------

import random
from ServiceEngine import ServiceContext
from MyPythonUtility.easy_config import EasyConfig
from Workflow.CommonFlowUtility import CrawlContext
from Workflow.CommonFeedsCrawFlow import build_crawl_ctx_by_config
from Workflow.IntelligenceCrawlFlow import (
    intelligence_crawler_result_handler,
    intelligence_crawler_fileter, \
    intelligence_crawler_exception_handler)

NAME = 'tass'
config: EasyConfig | None = None
crawl_context: CrawlContext | None = None


def module_init(service_context: ServiceContext):
    global config
    global crawl_context
    config = service_context.config
    crawl_context = build_crawl_ctx_by_config(NAME, config)


def start_task(stop_event):
    # Crawl main process
    run_pipeline(
        article_filter=partial(intelligence_crawler_fileter, context=crawl_context, levels=[NAME]),
        content_handler=partial(intelligence_crawler_result_handler, context=crawl_context, levels=[NAME]),
        exception_handler=partial(intelligence_crawler_exception_handler, context=crawl_context, levels=[NAME])
    )
    # Check and submit cached data.
    crawl_context.submit_cached_data(10)
    # Randomly delay for next crawl.
    CrawlContext.wait_interruptibly(random.randint(10, 15) * 60, stop_event)

# --------------------------------------------- Manual Code End ---------------------------------------------
